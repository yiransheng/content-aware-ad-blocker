{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Display matplotlib plots in the output\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.svm import *\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "import sys\n",
    "from scipy.sparse import coo_matrix, vstack\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score,recall_score,precision_score,classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "5942 items in raw table.\n",
      "5942 unique scripts.\n",
      "2971 positive + 2971 negative examples = 5942 total.\n",
      "Training sizes: [375, 750, 1500, 3000]\n",
      "Test size: 2942\n",
      "Chi2 Feature Selection Percentage: 0.50 \n",
      " Latent Dirchilet Allocation # Topis: 800\n",
      "--------------------------------------------------\n",
      "RandomForest\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.92      0.87      1471\n",
      "          1       0.91      0.81      0.85      1471\n",
      "\n",
      "avg / total       0.87      0.86      0.86      2942\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "REPO_ROOT = \"/Users/samuelkahn/Desktop/Berkeley/DS210\"\n",
    "\n",
    "def tokenize_js(script):\n",
    "    script = re.sub(r'(\\/\\*[^*]+\\*\\/)', \"\", script)\n",
    "    script = re.sub(r'\\/\\/.+', \"\", script)\n",
    "    tokens = re.findall(r'(\\\"[^\"]+\\\"|\\'[^\\']+\\'|[\\w\\\\\\/\\-_\\\"\\']+|{|}|,|[\\+\\*]|\\(|\\)|\\.|/\\*.+\\*\\/)', script)\n",
    "    return [t.lower() for t in tokens]\n",
    "\n",
    "def parse_scripts(tbl):\n",
    "    scripts=[]\n",
    "    if isinstance(tbl,dict) and 3000 in tbl.keys():\n",
    "        for item in tbl[3000]:\n",
    "            with open(\"%s/scripts_min/%s.js\" % (REPO_ROOT, item[\"sha\"])) as f:     \n",
    "                scripts.append(f.read().decode(errors='replace'))\n",
    "        return scripts\n",
    "    else:\n",
    "        for item in tbl:\n",
    "            with open(\"%s/scripts_min/%s.js\" % (REPO_ROOT, item[\"sha\"])) as f:     \n",
    "                scripts.append(f.read().decode(errors='replace'))\n",
    "        return scripts\n",
    "\n",
    "\n",
    "def topic_model_train(tokenizer,topics):\n",
    "    X_train = {}\n",
    "    Y_train = {}\n",
    "    X_test = {}\n",
    "    Y_test = {}\n",
    "    \n",
    "    script_list = [tokenizer(script) for script in parse_scripts(train_tables)]\n",
    "\n",
    "    frequency = defaultdict(int)\n",
    "    for text in script_list:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    train_texts = [[token for token in text if frequency[token] > 1] for text in script_list]\n",
    "\n",
    "\n",
    "    train_dictionary = corpora.Dictionary(train_texts)\n",
    "    train_corpus = [train_dictionary.doc2bow(text) for text in train_texts]\n",
    "\n",
    "    ### Create and save TFIDF representation of the data\n",
    "    tfidf = models.TfidfModel(train_corpus)\n",
    "\n",
    "    train_tfidf = tfidf[train_corpus]\n",
    "    lda = models.ldamodel.LdaModel(train_tfidf, id2word=train_dictionary, num_topics=topics,passes=20)\n",
    "    \n",
    "    #### create test set \n",
    "    script_list_test = [tokenizer(script) for script in parse_scripts(test_table)]\n",
    "\n",
    "    frequency = defaultdict(int)\n",
    "    for text in script_list:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    test_texts = [[token for token in text if frequency[token] > 1] for text in script_list_test]\n",
    "\n",
    "\n",
    "    test_vectors = map(lambda x: train_dictionary.doc2bow(x),test_texts)\n",
    "    test_tfidf = tfidf[test_vectors]\n",
    "\n",
    "    lda_train=lda[train_tfidf]\n",
    "\n",
    "    lda_test=lda[test_tfidf]\n",
    "\n",
    "    train_vector = gensim.matutils.corpus2csc(lda_train).todense().transpose()\n",
    "    test_vector = gensim.matutils.corpus2csc(lda_test).todense().transpose()\n",
    "    \n",
    "\n",
    "    return train_vector,test_vector\n",
    "\n",
    "def vectorize_table(model_type, tokenizer, parser, table, train_size, test_size):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenizer)\n",
    "    \n",
    "    data = {\n",
    "        \"X_train\": vectorizer.fit_transform(parser(table[:train_size])),\n",
    "        \"Y_train\": np.array([item[\"flag-any\"] for item in table[:train_size]]),\n",
    "        \"X_test\": vectorizer.transform(parser(table[-test_size:])),\n",
    "        \"Y_test\": np.array([item[\"flag-any\"] for item in table[-test_size:]]),\n",
    "    }\n",
    "    \n",
    "    with open(\"%s/models/data/dataset_%s_%d.pickle\" % (REPO_ROOT, model_type, train_size), \"w\") as f:\n",
    "        pickle.dump(data, f)    \n",
    "def test_model(model, model_name,train_vector,test_vector,target_train,target_test):\n",
    "    model.fit(train_vector, target_train)\n",
    "    test_pred = model.predict(test_vector)\n",
    "    print '-'*50\n",
    "    print model_name\n",
    "    print classification_report(target_test,test_pred)\n",
    "    print '-'*50\n",
    "def write_results():\n",
    "    output = (\"Model,Training set,Accuracy,Precision,Recall,F1 score\\n\" +\n",
    "        \"\\n\".join([\",\".join([str(s) for s in row]) for row in output_table]))\n",
    "    with open(\"%s/results/linear_models.csv\" % REPO_ROOT, \"w\") as f:\n",
    "        f.write(output)\n",
    "        \n",
    "def write_min_dataset():\n",
    "    min_table = (\n",
    "            [table[i] for i in positive_examples] + \n",
    "            [table[i] for i in negative_examples])\n",
    "    with open(\"%s/scripts_min/min/table_flag.json\" % REPO_ROOT, \"w\") as f:\n",
    "        json.dump(min_table, f)\n",
    "    \n",
    "    for item in min_table:\n",
    "        with open(\"%s/scripts/%s.js\" % (REPO_ROOT, item[\"sha\"])) as f1:\n",
    "            with open(\"%s/scripts/min/%s.js\" % (REPO_ROOT, item[\"sha\"]), \"w\") as f2:\n",
    "                f2.write(f1.read())\n",
    "    print \"Wrote %d items\" % len(min_table)\n",
    "\n",
    "with open(\"%s/scripts_min/table_flag.json\" % REPO_ROOT) as f:\n",
    "    raw_table = json.load(f)\n",
    "\n",
    "# Filter out inline scripts for now\n",
    "raw_table = [i for i in raw_table if i[\"inline\"] == False]\n",
    "\n",
    "scripts_table = {}\n",
    "for item in raw_table:\n",
    "    if item[\"sha\"] not in scripts_table:\n",
    "        scripts_table[item[\"sha\"]] = item\n",
    "        scripts_table[item[\"sha\"]][\"count\"] = 0\n",
    "        \n",
    "    for entry in item:\n",
    "        if entry.startswith(\"flag-\"):\n",
    "            if item[entry] == 1:\n",
    "                scripts_table[item[\"sha\"]][entry] = 1\n",
    "                \n",
    "    scripts_table[item[\"sha\"]][\"count\"] += 1\n",
    "    \n",
    "table = scripts_table.values()\n",
    "\n",
    "positive_examples = [i for i, e in enumerate(table) if e[\"flag-any\"] == 1]\n",
    "negative_examples = [i for i, e in enumerate(table) if e[\"flag-any\"] == 0]\n",
    "random.seed(1492)\n",
    "random.shuffle(positive_examples)\n",
    "random.shuffle(negative_examples)\n",
    "negative_examples = negative_examples[:len(positive_examples)]\n",
    "TOTAL_SIZE = len(positive_examples)+len(negative_examples)\n",
    "print \"%d items in raw table.\" % len(raw_table)\n",
    "print \"%d unique scripts.\" % len(table)\n",
    "print \"%d positive + %d negative examples = %d total.\" % (\n",
    "    len(positive_examples), len(negative_examples),\n",
    "    TOTAL_SIZE)\n",
    "\n",
    "TRAIN_SIZES = []\n",
    "size = 375\n",
    "while size < TOTAL_SIZE - 1000:\n",
    "    TRAIN_SIZES.append(size)\n",
    "    size *= 2\n",
    "    \n",
    "TEST_SIZE = TOTAL_SIZE - TRAIN_SIZES[-1]\n",
    "\n",
    "print \"Training sizes: %s\" % TRAIN_SIZES\n",
    "print \"Test size: %d\" % TEST_SIZE\n",
    "\n",
    "train_tables = {}\n",
    "for train_size in TRAIN_SIZES:\n",
    "    train_tables[train_size] = (\n",
    "        [table[i] for i in positive_examples[:(train_size/2)]] + \n",
    "        [table[i] for i in negative_examples[:(train_size/2)]])\n",
    "\n",
    "test_table = (\n",
    "    [table[i] for i in positive_examples[-TEST_SIZE/2:]] + \n",
    "    [table[i] for i in negative_examples[-TEST_SIZE/2:]])\n",
    "\n",
    "X_train = {}\n",
    "Y_train = {}\n",
    "X_test = {}\n",
    "Y_test = {}\n",
    "\n",
    "script_list = [tokenize_js(script) for script in parse_scripts(train_tables)]\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in script_list:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "train_texts = [' '.join([token for token in text if frequency[token] > 0]) for text in script_list]\n",
    "target_train = np.array([item[\"flag-any\"] for item in train_tables[3000]])\n",
    "target_test = np.array([item[\"flag-any\"] for item in test_table])\n",
    "\n",
    "\n",
    "script_list_test = [tokenize_js(script) for script in parse_scripts(test_table)]\n",
    "\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in script_list:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "test_texts = [' '.join([token for token in text if frequency[token] > 0]) for text in script_list_test]\n",
    "# for percent in [0.1,0.2,0.3,0.4,0.5]:\n",
    "#     for topics in [100,200,400,800]:\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize_js)\n",
    "train_data=vectorizer.fit_transform(train_texts)\n",
    "test_data=vectorizer.transform(test_texts)\n",
    "chi_fs=SelectPercentile(chi2,0.4)\n",
    "chi2_train=chi_fs.fit_transform(train_data,target_train).todense()\n",
    "chi2_test=chi_fs.transform(test_data).todense()\n",
    "train_lda,test_lda=topic_model_train(tokenize_js,100)\n",
    "\n",
    "train=np.hstack([chi2_train,train_lda])\n",
    "test=np.hstack([chi2_test,test_lda])\n",
    "\n",
    "\n",
    "print 'Chi2 Feature Selection Percentage: %.2f \\n Latent Dirchilet Allocation # Topis: %i'%(percent,topics)\n",
    "test_model(RandomForestClassifier(n_estimators=1000),'RandomForest',train,test,target_train,target_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
